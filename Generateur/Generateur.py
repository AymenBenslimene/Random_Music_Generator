# -*- coding: utf-8 -*-
"""Generateur.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k3rAKLNkGFNTHTOHDy4BNcdlg_BLsdMd

**PROJET GENERATEUR ALEATOIRE DE MUSIQUE A L'AIDE D'APPRENTISSAGE PROFOND**
 **2021/2022**

**ETUDIANTS INGENIEURS à SUPGALILEE:** 
- BEN SLIMENE Aymen
- GHEDES Hala
- OUELBANI Ahmed Amin
- KHELIFA Amel
- SAIDI Dorra
"""

#PREPARATION DES DONNEES ET ENTRAINEMENET DE MODELE 

##INSTALLATION
!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev
!pip install -qU pyfluidsynth pretty_midi
!pip install music21
!pip install pypianoroll

##IMPORTATION 

import os
import shutil
import glob
import numpy as np 
import pandas as pd
import pretty_midi
import pypianoroll
import tables
from music21 import converter, instrument, note, chord, stream
import music21
import librosa
import librosa.display
import matplotlib.pyplot as plt
from keras.utils import np_utils
import json
import IPython.display
from datetime import datetime

import random
import itertools

##DEFINITION DES PATHS POUR ACCEDER A LA BASE DE DONNEES

#root_dir = 'drive/MyDrive/Groupe6RMG'
root_dir ='/content/drive/MyDrive/Groupe6RMG'
data_dir = root_dir + '/Lakh Piano Dataset/LPD-5/lpd_5/lpd_5_cleansed'
music_dataset_lpd_dir = root_dir + '/Music Dataset/midis/lmd_matched/lmd_matched'

##CONNCETER AVEC GOOGLE DRIVE
from google.colab import drive
drive.mount('/content/drive')

"""**EXTRAIRE LES FICHIERS MIDI DE LA BASE DE DONNEES**"""

RESULTS_PATH = os.path.join(root_dir, 'Lakh Midi Dataset', 'lmd_matched_h5')
#Des fonction pour extraire les MIDI de la base données (Prédifini lors de l'utilisation de la base de donnée LMD)
def msd_id_to_dirs(msd_id):
    """Given an MSD ID, generate the path prefix.
    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678"""
    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)
#def msd_id_to_h5(msd_id):
    #"""Given an MSD ID, return the path to the corresponding h5"""
    #return os.path.join(RESULTS_PATH, 'lmd_matched_h5',
                        #msd_id_to_dirs(msd_id) + '.h5')
def msd_id_to_h5(msd_id):
    """Given an MSD ID, return the path to the corresponding h5"""
    return os.path.join(RESULTS_PATH,msd_id_to_dirs(msd_id) + '.h5')                        
# Load the midi npz file from the LMD cleansed folder
def get_midi_npz_path(msd_id, midi_md5):
    return os.path.join(data_dir,
                        msd_id_to_dirs(msd_id), midi_md5 + '.npz')
    # Load the midi file from the Music Dataset folder
def get_midi_path(msd_id, midi_md5):
    return os.path.join(music_dataset_lpd_dir,
                        msd_id_to_dirs(msd_id), midi_md5 + '.mid')
    
#### Charger des fichiers relié à la base de données 

# Open the cleansed ids - cleansed file ids : msd ids
cleansed_ids = pd.read_csv(os.path.join('/content/drive/MyDrive/Groupe6RMG', 'Lakh Piano Dataset', 'cleansed_ids.txt'), delimiter = '    ', header = None)
lpd_to_msd_ids = {a:b for a, b in zip(cleansed_ids[0], cleansed_ids[1])}
msd_to_lpd_ids = {a:b for a, b in zip(cleansed_ids[1], cleansed_ids[0])}

### METHODES pour extraire les metadata et les fichier MIDI

#test
lpd_to_msd_ids['904017ae5e1ceb7ab2c17022c8db49d9']
msd_to_lpd_ids['TRALLSG128F425A685']
#test
msd_id1='TRALLSG128F425A685'
lpd_id='904017ae5e1ceb7ab2c17022c8db49d9'
print(lpd_to_msd_ids['904017ae5e1ceb7ab2c17022c8db49d9'])


#####EXECUTED ONE TO SAVE JSON ###########
##for i in range(0,len(lmdkeys)):
 
    ##try:
      #print(lpd_to_msd_ids[lmdkeys[i]])
     ## with tables.open_file(msd_id_to_h5(lpd_to_msd_ids[lmdkeys[i]])) as h5:
      ##  artistname=str(h5.root.metadata.songs.cols.artist_name[0])
       ## artistname=artistname[2:len(artistname)-1]
        ##title=format(h5.root.metadata.songs.cols.title[0])
        ##title=title[2:len(title)-1]
       ## lmd_metadata1.append({'msd_id':lpd_to_msd_ids[lmdkeys[i]],'artist':artistname,'title':title})
        
   ## except:
    ##  continue
#print(lmd_metadata1)
##with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'processed_metadata.json'), 'w') as outfile:
  ##json.dump(lmd_metadata1, outfile)

## Charger le fichier JSON pour charger les fichiers midi

# Loading the files
with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'processed_metadata.json'), 'r') as outfile:
  lmd_metadata1 = json.load(outfile)
#convertir en dictionnaire 
lmd_metadata1 = {e['msd_id']:e for e in lmd_metadata1}

"""**Explorer la base de données**"""

from collections import Counter
song_artists = {k:v['artist'] for k, v in lmd_metadata1.items()}
print(song_artists)

song_artists_counter = Counter(song_artists.values())
song_artists_counter.most_common(30)

# Filter by artist name
[(v['title'], v['artist']) for k, v in lmd_metadata1.items() if 'Panpipes' in v['artist']]

"""**Choix des fichiers Midi: soit aléatoirement, soit par artiste afin de créer le modèle**"""

# Get all song MSD IDs satisfying the condition
#filtered_msd_ids = [k for k, v in lmd_metadata1.items() if 'Michael Jackson' in v['artist']]
## ICI nous avons utiliser 2000 fichier midi ( au total il ya à peu prés 12000 fichiers midi voir plus)
filtered_msd_ids = [k for k, v in lmd_metadata1.items() if 'Michael Jackson' in v['artist'] ]
#filtered_msd_ids=filtered_msd_ids[:2000]
print (len(filtered_msd_ids))

# choix de 1000 fichier midi par defaut
train_ids = random.choices(filtered_msd_ids, k = 1000)

## On va extraire les fichiers midi selon les IDs 

msd_id = filtered_msd_ids[0]
print(msd_id)

lpd_file_name = msd_to_lpd_ids[msd_id]
npz_path = get_midi_npz_path(msd_id, lpd_file_name)
print(npz_path)
multitrack = pypianoroll.load(npz_path)
pm = pypianoroll.to_pretty_midi(multitrack)
new_midi_path = npz_path[:-4] + '.mid'
pypianoroll.write(new_midi_path, multitrack)
# Get the MIDI path (should already be generated)
new_midi_path = npz_path[:-4] + '.mid'
midi = converter.parse(new_midi_path)
parts = list(instrument.partitionByInstrument(midi))

###Pour chaque fichier Midi nous allons choisir un seule instrument et extraire pour chaque fichier midi les notes et l'hauteur( pitch )

notes_song = []
elements_song = []
for part in parts:
  notes_part = []
  elements_part = []
  for element in part:
    elements_part.append(element)
    if isinstance(element, note.Note):
      # Return the pitch of the single note
        notes_part.append(str(element.pitch))
    elif isinstance(element, chord.Chord):
      # Returns the normal order of a Chord represented in a list of integers
        notes_part.append('.'.join(str(n) for n in element.normalOrder))

  notes_song.append(notes_part)
  elements_song.append(elements_part)



# Loop that reads each song in train_ids, parses the PIANO notes and saves the string representation of the note in notes
notes = []

i = 0
for msd_file_name in filtered_msd_ids:
  lpd_file_name = msd_to_lpd_ids[msd_file_name]

  # Get the NPZ path
  npz_path = get_midi_npz_path(msd_file_name, lpd_file_name)

  multitrack = pypianoroll.load(npz_path)
  pm = pypianoroll.to_pretty_midi(multitrack)
  new_midi_path = npz_path[:-4] + '.mid'
  pypianoroll.write(new_midi_path, multitrack)
  # Get the MIDI path (should already be generated)
  new_midi_path = npz_path[:-4] + '.mid'
  midi = converter.parse(new_midi_path)

  s2 = instrument.partitionByInstrument(midi)
  piano_part = None
  # Filter for  only the piano part
  instr = instrument.Piano
  for part in s2:
    if isinstance(part.getInstrument(), instr):
      piano_part = part

  notes_song = []
  if piano_part: # Some songs somehow have no piano parts
    for element in piano_part:
      if isinstance(element, note.Note):
        # Return the pitch of the single note
          notes_song.append(str(element.pitch))
      elif isinstance(element, chord.Chord):
        # Returns the normal order of a Chord represented in a list of integers
          notes_song.append('.'.join(str(n) for n in element.normalOrder))

  notes.append(notes_song)
  i+=1
print(i)
print(notes)

print(new_midi_path)

"""**PARTIE GENERATION**

**Préparation de l'entrainement do modèle**
"""

#### DIVISIER 
# Random Train test split 
random.seed(42)
test_ids = random.choices(list(range(len(notes))), k = len(notes))
train_ids = [e for e in range(len(notes)) if e not in test_ids]

notes_train = [notes[i] for i in train_ids]

notes_test = [notes[i] for i in test_ids]

with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'notes_initial_train.json'), 'w') as outfile:
  json.dump(notes_train, outfile)

with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'notes_initial_test.json'), 'w') as outfile:
  json.dump(notes_test, outfile)

## On les charge 

# Loading the files
with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'notes_initial_train.json'), 'r') as outfile:
  notes_train = json.load(outfile)

with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'notes_initial_test.json'), 'r') as outfile:
  notes_test = json.load(outfile)

print(len(train_ids))
print(len(test_ids))

print(train_ids)
print(test_ids)

print(len(notes))

"""
**Preparation des entrées et sorties de la séquence**
"""

# Prepare input and output sequences

def prepare_sequences(notes, note_to_int = None, sequence_length = 32):
  network_input = []
  network_output = []

  if not note_to_int:
    # Set of note/chords (collapse into list)
    pitch_names = sorted(set(itertools.chain(*notes)))
    # create a dictionary to map pitches to integers
    note_to_int = dict((note, number) for number, note in enumerate(pitch_names))

  # Loop through all songs
  for song in notes:
    # Check for the end
    i = 0
    while i + sequence_length < len(song):
      # seq_len notes for the input seq
      sequence_in = song[i: i + sequence_length]
      # Next note to predict
      sequence_out = song[i+sequence_length]
      # Return the int representation of the note - *(If note not found)
      network_input.append([note_to_int.get(char, 0) for char in sequence_in])
      network_output.append(note_to_int.get(sequence_out, 0))
      i += sequence_length

  n_patterns = len(network_input)
  print("Taille de l'input", n_patterns)
  # Reshape for LSTM input
  network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))
  # Normalize input (?? - CHECK LATER - this assumes the alphabetical order of the notes carries semantic meaning?)
  #network_input = network_input / len(pitch_names)
  #network_output = np_utils.to_categorical(network_output)

  return network_input, network_output, note_to_int

train_input, train_output, note_to_int = prepare_sequences(notes_train, sequence_length = 64)
test_input, test_output, _ = prepare_sequences(notes_test, note_to_int = note_to_int, sequence_length = 64)

## CONVERTIR ENTIER EN MIDI 
int_to_note = {number:note for note, number in note_to_int.items()}
print(int_to_note)

"""**PREPARATION DE GENERATEUR RNN LSTM**

Ici, nous definisson notre modèle, les fonctions d'optimisation, grad_clipping et d'entrainement de modèle
"""

## IMPORTATION 
import torch
import torch.nn as nn
from torch.nn import functional as F

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

## Fonction GRAD_CLIPPING

def grad_clipping(net, theta):  
    """Clip the gradient."""
    params = [p for p in net.parameters() if p.requires_grad]

    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm

## CLASS GENERATEUR

class GenerationRNN(nn.Module):
  # input_size: number of possible pitches
  # hidden_size: embedding size of each pitch
  # output_size: number of possible pitches (probability distribution)
    def __init__(self, input_size, hidden_size, output_size, n_layers=1):
        super(GenerationRNN, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)
        self.decoder = nn.Linear(hidden_size * n_layers, output_size)
    
    def forward(self, input, hidden):
        # Creates embedding of the input texts
        #print('initial input', input.size())
        input = self.embedding(input.view(1, -1))
        #print('input after embedding', input.size())
        output, hidden = self.gru(input, hidden)
        #print('output after gru', output.size())
        #print('hidden after gru', hidden.size())
        output = self.decoder(hidden.view(1, -1))
        #print('output after decoder', output.size())
        return output, hidden

    def init_hidden(self):
        return torch.zeros(self.n_layers, 1, self.hidden_size).to(device)

# FONCTION d'ENTRAINEMENT POUR UNE SEULE SEQUENECE 
def train_sequence(input, target, model, optimizer, criterion):
    # Initialize hidden state, zero the gradients of model 
    hidden = model.init_hidden()
    model.zero_grad()
    loss = 0
    # For each character in our chunk (except last), compute the hidden and ouput
    # Using each output, compute the loss with the corresponding target 
    for i in range(len(input)):
        output, hidden = model(input[i], hidden)
        loss += criterion(output, target[i].unsqueeze(0))
    
    # Backpropagate, clip gradient and optimize
    loss.backward()
    grad_clipping(model, 1)
    optimizer.step()

    # Return average loss for the input sequence
    return loss.data.item() / len(input)

# FONCTION DE TEST POUR UNE SEULE SEQUENECE 
def test_sequence(input, target, model, criterion):
    # Initialize hidden state, zero the gradients of model 
    hidden = model.init_hidden()
    model.zero_grad()
    loss = 0
    # For each character in our chunk (except last), compute the hidden and ouput
    # Using each output, compute the loss with the corresponding target 
    for i in range(len(input)):
        output, hidden = model(input[i], hidden)
        loss += criterion(output, target[i].unsqueeze(0))

    # Return average loss for the input sequence
    return loss.data.item() / len(input)

# FONCTION D'entrainement en boucle 

# Overall training loop
def training_loop(model, optimizer, scheduler, criterion, train_input, test_input):

  train_losses = []
  test_losses = []

  for epoch in range(1, n_epochs + 1):
    running_loss = 0
    model.train()

    # Training - sample 2000
    sampled_train_ids = random.choices(range(train_input.shape[0]), k = 2000)
    print(scheduler.get_last_lr())
    for i in range(train_input.shape[0]):
      sequence = train_input[i, : , :]
      input = torch.tensor(sequence[:-1], dtype = torch.long).squeeze().to(device)
      target = torch.tensor(sequence[1:], dtype = torch.long).squeeze().to(device)
      loss = train_sequence(input, target, model, optimizer, criterion)
      running_loss += loss

    train_epoch_loss = running_loss / 2000
    train_losses.append(train_epoch_loss)
    scheduler.step()

    running_loss = 0
    # model.eval()
    # # Testing
    # for i in range(test_input.shape[0]):
    #   sequence = test_input[i, : , :]
    #   input = torch.tensor(sequence[:-1], dtype = torch.long).squeeze().to(device)
    #   target = torch.tensor(sequence[1:], dtype = torch.long).squeeze().to(device)
    #   loss = test_sequence(input, target, model, criterion)
    #   running_loss += loss

    # test_epoch_loss = running_loss / 1000
    # test_losses.append(test_epoch_loss)
    test_epoch_loss = 0

    print('Epoch {}, Train Loss: {}, Test Loss: {}, Time: {}'.format(epoch, train_epoch_loss, test_epoch_loss, datetime.now()))

  return train_losses, test_losses

"""**Debut d'entrainement de modèle**

Dans cette partie, nous allons faire entrer des parametres differents pour chaque modèle et nous avons choisir le modèle ayant les meilleurs résultats

"""

## ON CHOISIT LES PARAMETRE EN ENTREE POUR NOTRE MODELE
n_pitches = len(note_to_int)
hidden_size = 96
n_layers = 2
n_epochs = 40
lr = 0.002
lr_lambda = 0.99

##MODELE
model = GenerationRNN(input_size = n_pitches, hidden_size = hidden_size, output_size = n_pitches, n_layers = n_layers).to(device)
##OPTIMISEUR
optimizer = torch.optim.Adam(model.parameters(), lr = lr)
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)
criterion = nn.CrossEntropyLoss()
train_losses, test_losses = training_loop(model, optimizer, scheduler, criterion, train_input, train_input)

#ENREGISTRER LE MODELE 

model_name = 'RNN_modele1'
save_path = os.path.join(root_dir, 'Saved Models', model_name)
torch.save(model.state_dict(), save_path)

## AUTRE ENTRAINEMENT SUCCESSIVE DE MODELE AVEC N EXPERIENCE


## ON CHOISIT POUR CHAQUE EXPERIENCE LES PARAMETRE AFIN DE DISTINGUER LES MEILLEURS RESULTATS

experiment_params_list = [
                          {'HIDDEN': 128, 'N_LAYERS': 2, 'LR': 0.001, 'LR_LAMBDA': 1, 'OPTIM': 'Adam'},
                          {'HIDDEN': 64, 'N_LAYERS': 4, 'LR': 0.001, 'LR_LAMBDA': 1, 'OPTIM': 'Adam'},
                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.05, 'LR_LAMBDA': 0.95, 'OPTIM': 'SGD'},
                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.025, 'LR_LAMBDA': 0.95, 'OPTIM': 'SGD'},
                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.001, 'LR_LAMBDA': 0.975, 'OPTIM': 'Adam'},
                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.0025, 'LR_LAMBDA': 0.95, 'OPTIM': 'Adam'},
                    ]

experiment_losses = {}
experiment_num = 0

for params in experiment_params_list:

  n_pitches = len(note_to_int)
  hidden_size = params['HIDDEN']
  n_layers = params['N_LAYERS']
  n_epochs = 50
  lr = params['LR']
  lr_lambda = params['LR_LAMBDA']

  print(experiment_num, params)

  # Create model, optimizer and loss function
  model = GenerationRNN(input_size = n_pitches, hidden_size = hidden_size, output_size = n_pitches, n_layers = n_layers).to(device)
  if params['OPTIM'] == 'Adam':
    optimizer = torch.optim.Adam(model.parameters(), lr = lr)
  else:
    optimizer = torch.optim.SGD(model.parameters(), lr = lr)

  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)
  criterion = nn.CrossEntropyLoss()
  train_losses, test_losses = training_loop(model, optimizer, scheduler, criterion, train_input, test_input)

  #ENREGISTRE LE MODELE SELON LE NOM ET NUMERO D'EXPERIENCE
  model_name = 'RNN_MODELE2_EXPERIENCE exp{}'.format(experiment_num)
  save_path = os.path.join(root_dir, 'Saved Models', model_name)
  torch.save(model.state_dict(), save_path)

  # Save experiment losses
  experiment_losses.update({experiment_num: {'train_losses': train_losses, 'test_losses': test_losses}})

  # Plot the losses over epochs
  plt.figure()
  plt.plot(train_losses, label = 'Train Loss')
  plt.plot(test_losses, label = 'Test Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()
  plt.title(experiment_num)
  plt.show()
  try:
    with open(os.path.join(root_dir, 'experiment_losses 20 Apr.json'), 'w') as outfile:
      json.dump(experiment_losses, outfile)
  except:
    print('failed to save')

  experiment_num += 1

## ENREGISTRE LE DERNIER MODELE ET EN AFFICHE LES STATISTIQUE

# Save Model
model_name = 'RNN_MODELE'
save_path = os.path.join(root_dir, 'Saved Models', model_name)
torch.save(model.state_dict(), save_path)

# Plot the losses over epochs
plt.figure()
plt.plot(train_losses, label = 'Train Loss')
plt.plot(test_losses, label = 'Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title(experiment_num)
plt.show()

"""**PARTIE DE GENERATION DES SEQUENCES**

Dans cette partie, nous avons generer le fichier MIDI, ceci est réalisé avec l'interface graphique de notre projet.

Si vous avez seulement besoin de créer le modèle, il n'est pas nécessaire d'exécuter cette partie.
"""

#FONCTION D'EVALUATION 

def evaluate(net, prime_seq, predict_len):
    '''
    Arguments:
    prime_seq - priming sequence (converted t)
    predict_len - number of notes to predict for after prime sequence
    '''
    hidden = net.init_hidden()

    predicted = prime_seq.copy()
    prime_seq = torch.tensor(prime_seq, dtype = torch.long).to(device)


    # "Building up" the hidden state using the prime sequence
    for p in range(len(prime_seq) - 1):
        input = prime_seq[p]
        _, hidden = net(input, hidden)
    
    # Last character of prime sequence
    input = prime_seq[-1]
    
    # For every index to predict
    for p in range(predict_len):

        # Pass the inputs to the model - output has dimension n_pitches - scores for each of the possible characters
        output, hidden = net(input, hidden)

        # Pick the character with the highest probability 
        predicted_id = torch.argmax(torch.softmax(output, dim = 1))

        # Add predicted index to the list and use as next input
        predicted.append(predicted_id.item()) 
        input = predicted_id

    return predicted

#FONCTION D'EVALUATION MULTINOMIAL

def evaluateMultinomial(net, prime_seq, predict_len, temperature=0.8):
    '''
    Arguments:
    prime_seq - priming sequence (converted t)
    predict_len - number of notes to predict for after prime sequence
    '''
    hidden = net.init_hidden()

    predicted = prime_seq.copy()
    prime_seq = torch.tensor(prime_seq, dtype = torch.long).to(device)


    # "Building up" the hidden state using the prime sequence
    for p in range(len(prime_seq) - 1):
        input = prime_seq[p]
        _, hidden = net(input, hidden)
    
    # Last character of prime sequence
    input = prime_seq[-1]
    
    # For every index to predict
    for p in range(predict_len):

        # Pass the inputs to the model - output has dimension n_pitches - scores for each of the possible characters
        output, hidden = net(input, hidden)
        # Sample from the network output as a multinomial distribution
        output = output.data.view(-1).div(temperature).exp()
        predicted_id = torch.multinomial(output, 1)

        # Add predicted index to the list and use as next input
        predicted.append(predicted_id.item()) 
        input = predicted_id

    return predicted

### FONCTION CREATION DE FICHIER MIDI A PARTIR D'une SEQUENCE DE NOTE 
def create_midi(prediction_output):
    """ convert the output from the prediction to notes and create a midi file
        from the notes """
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)

    return midi_stream

## NOUS ALLONS MAINTENANT GENERER UNE SEQUENECE ET ENSUITE NOUS ALLONS LA CONVERTIR EN NOTES ET CREER UN FICHIER MIDI

############# GENERER LA SEQUENECE ###############
#on choisit une séquence primale 
sequence_primal=[100, 101, 102, 101, 100, 101, 102, 101, 10]
generated_seq = evaluate(model,sequence_primal , predict_len = 500)
generated_seq_multinomial = evaluateMultinomial(model, sequence_primal, predict_len = 500, temperature = 1)
#on affiche la sequence genere avec la fonction d'evaluation argmax et la fonction d'evaluation multinomial
print(generated_seq)
print(generated_seq_multinomial)

############# CONVERTIR LA SEQUENCE GENEREE EN NOTE ###############

generated_seq = [int_to_note[e] for e in generated_seq]
generated_seq_multinomial = [int_to_note[e] for e in generated_seq_multinomial]

############# CREATION DE FICHIER MIDI ##################

name="MIDIGENERATED"
generated_path = os.path.join(root_dir, 'Generated MIDIs', name)
generated_stream = create_midi(generated_seq_multinomial)
generated_stream.write('midi', fp=generated_path)

############## CHARGER LE FICHIER MIDI ############
generated_multitrack = pypianoroll.read(generated_path)
generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)
generated_midi_audio = generated_pm.fluidsynth()
IPython.display.Audio(generated_midi_audio, rate = 44100)


########## VISUALISER LES NOTES DE FICHIER MIDI 

# Load the Npz file into a multitrack object
#multitrack = pypianoroll.load(generated_multitrack)
# Load a specific track from the multitrack
#track = multitrack.tracks[1]
# Visualize the track (can also do pypianoroll.plot_track(track))
#track.plot()

########## VISUALISER LES NOTES DE FICHIER MIDI 

# Load the Npz file into a multitrack object
#multitrack = pypianoroll.load(generated_multitrack)
# Load a specific track from the multitrack
#track = multitrack.tracks[1]
# Visualize the track (can also do pypianoroll.plot_track(track))
#track.plot()

multitrack = pypianoroll.read(generated_path)
print(multitrack)
multitrack.trim(0, 12 * multitrack.resolution)
multitrack.binarize()
multitrack.plot()

